{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.onnx\n",
        "from transformers import ViTConfig, ViTForImageClassification\n",
        "import torch.nn as nn\n",
        "import onnx\n",
        "from onnxsim import simplify\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType"
      ],
      "metadata": {
        "id": "TDH1UvdnOWHm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cpu\")\n",
        "class_names = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
        "num_classes = 7\n",
        "\n",
        "config = ViTConfig(num_labels=num_classes)\n",
        "model = ViTForImageClassification.from_pretrained(\n",
        "    \"google/vit-base-patch16-224-in21k\",\n",
        "    config=config,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "in_features = model.classifier.in_features\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.Dropout(p=0.4),\n",
        "    nn.Linear(in_features, num_classes)\n",
        ")\n",
        "\n",
        "checkpoint = torch.load('/content/best_vit.pth', map_location=device, weights_only=False)\n",
        "\n",
        "if isinstance(checkpoint, dict):\n",
        "    if 'model_state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    elif 'state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['state_dict'])\n",
        "    else:\n",
        "        model.load_state_dict(checkpoint)\n",
        "else:\n",
        "    model.load_state_dict(checkpoint)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "torch.onnx.export(\n",
        "    model,\n",
        "    dummy_input,\n",
        "    \"emotion_vit_model.onnx\",\n",
        "    export_params=True,\n",
        "    opset_version=12,\n",
        "    do_constant_folding=True,\n",
        "    input_names=['pixel_values'],\n",
        "    output_names=['logits'],\n",
        "    dynamic_axes={\n",
        "        'pixel_values': {0: 'batch_size'},\n",
        "        'logits': {0: 'batch_size'}\n",
        "    }\n",
        ")\n",
        "\n",
        "onnx_model = onnx.load(\"emotion_vit_model.onnx\")\n",
        "onnx.checker.check_model(onnx_model)\n"
      ],
      "metadata": {
        "id": "SEahe6Lzf9h-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = onnx.load(\"emotion_vit_model.onnx\")\n",
        "model_simp, check = simplify(model)\n",
        "\n",
        "if check:\n",
        "    print(\"Model simplified successfully\")\n",
        "    onnx.save(model_simp, \"emotion_vit_model_simplified.onnx\")\n",
        "    quantize_dynamic(\n",
        "        \"emotion_vit_model_simplified.onnx\",\n",
        "        \"emotion_vit_model_quantized.onnx\",\n",
        "        weight_type=QuantType.QUInt8\n",
        "    )\n"
      ],
      "metadata": {
        "id": "GbDVu0j0wxLf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}